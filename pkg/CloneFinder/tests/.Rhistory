# As described below (in the main function), the idea is to combine the observed
# read-data with the prior on the fraction nu and on the discrete (hidden)
# copy number & snp states to get the most likely variant-type (SNP or Mutation)
# for each variant.  Well, technically, get the 'maximum a posteriori' value....
computeMAPestimateCNV <- function(hiddenloglike, prior, nms) {
# First, we need to convert likelihoods to posteriors. We multiply by the
# prior distributions -- which translates to addition on the log scale.
# One part comes from the discrete (hidden) prior.
pd <- prior@discrete
foo <- as.matrix(log(pd[, nms ]))
logpost <- sweep(hiddenloglike, 2:3, foo, "+")
# The other part comes from the beta prior on nu.
nuprior <- prior@pdf
# assumes the grid is the same both places
logpost <- sweep(logpost, 1, nuprior, "+")
# Now we have to normalize so the sum over all posterior
# probabilities equals one -- for each observed datarow
weights <- log( apply(exp(logpost), 3, sum) )
logpost<- sweep(logpost, 3, weights, '-')
hiddenpost <- exp(logpost)
# At this point, 'hiddenpost' is the posterior probability
#     Prob(Vi=v, nu=nu0, S=s | Ki=k, Ni=n)
# Brute force method to find max a poateriori values
temp <- apply(hiddenpost, 2:3, sum)
idx <- sapply(levels(pd$S), function(L) {
w <- which(pd$S==L)
m <- apply(temp[w,,drop=FALSE], 2, which.max)
w[m]
})
if (!inherits(idx, "matrix")) {
nm <- names(idx)
idx <- matrix(idx, nrow=1)
colnames(idx) <- nm
}
calls <- pd[idx, "Valt"]
list(Call=calls, Index=idx, Support=temp)
}
computePosterior <- function(loglike, prior) {
nup <- prior@pdf
pop <- sweep(loglike, 1, nup, "+")
# assumes the same answer for pd$Mutation and for pd$SNP
pop <- sweep(pop, 2, log(cnPrior(prior)), "+")
# normalize so the sum of the posterior probabilities equals one
weights <- log( apply(exp(pop), 3, sum) )
pop<- sweep(pop, 3, weights, '-')
exp(pop)
}
# the 'obs' input must be a data.frame with columns labeled
#      K = number of variant reads
#      N = total number of reads
#      V = type of variant  (SNP or Mutation)
# KRC: should we force this to be an S4-object so we can perform
# validity checking?
makeCNVPosterior <- function(obs, prior) {
# need a three-dimensional array
#  1. grid points over the beta distribution on nu
#  2. copy number choices * variant type choices
#  3. number of data rows in 'obs'
gridPoints <- prior@grid
pd <- prior@discrete
nms <- paste(pd$S, pd$Valt, sep='.')
hiddenloglike <- array(NA,
dim=c(length(gridPoints), nrow(prior@discrete), nrow(obs)),
dimnames=list(NULL, nms, rownames(obs)))
# we get the log likelihood by iterating over every possible value
for (i in 1:nrow(obs)) {   # each data row
for (pj in 1:nrow(pd)) { # each hidden discrete state
cn <- as.character(pd[pj, "S"])
vtype <- as.character(pd[pj, "Valt"])
temp <- sapply(gridPoints, cnvLikelihood, S=cn, V=vtype, K=obs$K[i], N=obs$N[i])
hiddenloglike[,pj,i] <- temp
}
}
# At this point, 'hiddenloglike' represents the logarithm of
#     Prob(Ki=k, Ni=n | Vi=v, nu=nu0, S=s)
# We also need something to represent the logarithm of
#     Prob(Kbar = kbar, Nbar=nbar | Vbar=vbar, nu=nu0, S=s)
# Since this probability is the product over datarows i, we
# might take the sum on the log scale
#    snploglike <- apply(hiddenloglike, 1:3, sum)
# but this isn't quite correct, since we still need to
# collapse it for each vector of variantType assignments.
# The problem is that, with 10 datarows, there are 4^10
# possible vector assignments to consider, and that's way
# too many to store and carry around with us.
#
# so, we are going to compute the one-at-a-time posterior
# probabilities that Vi=v from the  above stuff and collapse
# down using the maximum a posteriori (MAP) estimate.  After
# that, we pretend that the MAP variant types are the truth.
nn <- computeMAPestimateCNV(hiddenloglike, prior,
as.character(obs$V)) # sets vbar
snploglike <- array(NA,
dim=c(length(gridPoints), length(cnSet), nrow(obs)),
dimnames=list(NULL, cnSet, rownames(obs)))
for (i in 1:nrow(obs)) {
for (cn in cnSet) {
snploglike[,cn,i] <- hiddenloglike[,nn$Index[i, cn], i]
}
}
# Now 'snploglike' is the likelihood
#    Prob(Ki=k, Ni=n | Vbar=vbar, nu=nu0, S=s)
# where we used, in vbar,  the MAP value of vi for each datarow. We
# can reapply the beta and copy number priors to start converting
# these likelihoods into posteriors..
snppost <- computePosterior(snploglike, prior)
# But we're stil not done. need to combine likelihoods across
# independent variants = datarows
loglike <- apply(snploglike, 1:2, sum)
tmp <- sweep(loglike, 1, prior@pdf, "+")
tmp <- sweep(tmp, 2, log(cnPrior(prior)), "+")
tmp <- exp(tmp)
posterior <- tmp/sum(tmp)
new("CNVPosterior",
loglike=loglike, posterior=posterior,
snploglike=snploglike, snppost=snppost,
hiddenloglike=hiddenloglike, calls=nn,
observed=obs, prior=prior)
}
#KRC: Why don't we have to export this class to get the 'summary' method for
# 'CNVPosterior' to work?
setClass("CNVPosteriorSummary",
representation=list(
copyNumbers = "data.frame",
loc="numeric", q05="numeric", q95="numeric",
grid="numeric", cdf='numeric', pdf='numeric'))
setMethod("as.data.frame", c("CNVPosteriorSummary"), function (x, row.names = NULL, optional = FALSE, ...) {
data.frame(x@copyNumbers, loc=x@loc, q05=x@q05, q95=x@q95)
})
setMethod("summary", c("CNVPosterior"), function(object, ...) {
# integrate out nu to get the copy number posteriors
cnpost <- as.data.frame(as.list(apply(object@posterior, 2, sum)))
# separately, have to work out the posterior on nu
# start by integrating out the copy number state
tp <- apply(object@posterior, 1, sum)
# find the mode
grid <- object@prior@grid
loc <- median(grid[tp == max(tp)])
cdf <- cumsum(tp)/sum(tp)
# TODO: Handle (literal) edge cases where MAP estimate is at
# one end of the interval
q05 <- round(max(grid[cdf < 0.05]), 3)
q95 <- round(min(grid[cdf > 0.95]), 3)
new("CNVPosteriorSummary", copyNumbers=cnpost,
loc=loc, q05=q05, q95=q95,
grid=grid, cdf=cdf, pdf=tp)
})
setMethod("show", c("CNVPosteriorSummary"), function(object) {
cat("Most likely normal fraction =", object@loc,
"\nNinety percent credible interval = (", object@q05, ',', object@q95, ")\n")
print(object@copyNumbers)
invisible(object)
})
setMethod("plot", c("CNVPosterior","missing"), function(x, place="topright", lwd=1, ...) {
post <- x@snppost
grid <- x@prior@grid
plot(grid, post[,"Normal",1], type="n", xlim=c(0,1),
ylim=range(post),
xlab="Normal Fraction", ylab="Posterior Probability",
lwd=lwd, ...)
n <- dim(post)[3]
c1p <- colorRampPalette(c("blue", "cyan"))(n)
c2p <- colorRampPalette(c("red", "magenta"))(n)
c3p <- colorRampPalette(c("green", "yellow"))(n)
for (i in 1:n) {
lines(grid, post[,"Normal",i], col=c1p[i], lwd=lwd)
lines(grid, post[,"Deleted",i], col=c2p[i], lwd=lwd)
lines(grid, post[,"Gained",i], col=c3p[i], lwd=lwd)
}
legend(place, c("Deleted", "Normal", "Gained"), lwd=2,
col=c("red", "blue", "green"))
invisible(x)
})
setMethod("hist", c("CNVPosterior"), function(x, place="topright", lwd=1, ...) {
post <- x@posterior
grid <- x@prior@grid
plot(grid, post[,"Deleted"], col="red", type="l",
ylim=range(c(post[,"Deleted"], post[,"Gained"], post[,"Normal"])),
xlab="Normal Fraction", ylab="Posterior Probability", lwd=lwd, ...)
lines(grid, post[,"Normal"], col="blue", lwd=lwd)
lines(grid, post[,"Gained"], col="green", lwd=lwd)
legend(place, c("Deleted", "Normal", "Gained"), lwd=2,
col=c("red", "blue", "green"))
invisible(post)
})
########################
# need to figure out what kind of data structures we need for deep sequencing
# of more than one gene
#
# Option 1: a list where each entry is the same input (data.frame) as provided
#           to makeCNVposterior
# Option 2: a data.frame like that provided to makeCNVPosterior but with an
#           extra column for 'Gene' identifier
# key point is that the estimates of normal fraction should borrow strength
# across all genes, but the copy number estimates should be gene-specific.
setClass("MultiCNVPosterior",
representation=list(
gposts = "list",
cps = "CNVPosteriorSummary"
))
setValidity("MultiCNVPosterior", function(object) {
all(unlist(lapply(object@gposts, inherits, what="CNVPosterior")))
})
multiCNVPosterior <- function(obs, prior=new("CNVPrior")) {
if (inherits(obs, 'data.frame')) { # input option 2
gcol <- which(colnames(obs) == "Gene")
if (length(gcol) == 0) { # assume only one gene
return(makeCNVPosterior(obs, prior))
} else {
gnames <- unique(obs$Gene)
obs <- lapply(gnames, function(g) obs[obs$Gene==g,])
}
}
# at this point, we're using input option #1, with a list of data.frames
posters <- lapply(obs, makeCNVPosterior, prior=prior)
# get posterior on nu by combining data over all genes
nupost <- Reduce("*", lapply(posters, function(po) {
apply(po@posterior, 1, sum)
}))
nupost <- nupost/sum(nupost)
# nupost is the pdf; also record the cdf
cdf <- cumsum(nupost)
# and the mode
prior <- posters[[1]]@prior
grid <- prior@grid
loc <- median(grid[nupost == max(nupost)])
# and the credible interval
q05 <- max(grid[cdf < 0.05])
q95 <- min(grid[cdf > 0.95])
# now go back and use the posterior distribuion of nu to
# get posteriors  of the per-gene copy number states
cn <- lapply(posters, function(po) {
tmp <- sweep(po@loglike, 1, nupost, "+")
tmp <- sweep(tmp, 2, log(cnPrior(prior)), "+")
tmp <- exp(tmp)
posterior <- tmp/sum(tmp)
apply(posterior, 2, sum)
})
cn <- as.data.frame(t(as.data.frame(cn)))
cps <- new("CNVPosteriorSummary", copyNumbers=cn,
loc=loc, q05=q05, q95=q95,
grid=grid, cdf=cdf, pdf=nupost)
new("MultiCNVPosterior", gposts=posters, cps=cps)
}
setMethod("summary", c("MultiCNVPosterior"), function(object, ...) {
object@cps
})
setMethod("plot", c("MultiCNVPosterior", "missing"), function(x, place="topright", lwd=1, ...) {
post <- x@cps@pdf
grid <- x@cps@grid
plot(grid, post, type="l",
xlab="Normal Fraction", ylab="Posterior Probability", lwd=lwd, ...)
invisible(post)
})
set.seed(1)
hclones <- 6
nclones <- 3
psis <- c(.5, .3, .2)
#l is the number of markers
l<- 30000
#The two 'alleles' are sets of markers along the paternal or maternal chromosome sets
allele.1 <- rep("A", l)
allele.2 <- rep("A", l)
#snprate is the rate of snps across the marker set
snprate <- .5
bset1 <- round(runif(round(l*snprate), 1, l))
bset2 <- round(runif(round(l*snprate), 1, l))
allele.1[bset1] <- "B"
allele.2[bset2] <- "B"
basesnp <- data.frame(allele.1, allele.2)
mu <- 5000
lossfun <- function(seq){
lvec <- 1:nrow(seq)
len <- round(rlnorm(1, log(mu), .5))
ind <- round(runif(1, 1, nrow(seq)-len))
allele <- sample(1, 1:ncol(seq))
newseq <- as.matrix(seq)
newseq[ind:(ind+len),allele] <- rep(0, len+1)
newseq
}
gainfun <- function(seq){
lvec <- 1:nrow(seq)
len <- round(rlnorm(1, log(mu), .5))
ind <- runif(1, 1, nrow(seq)-len)
allele <- sample(1, 1:ncol(seq))
newseq <- as.matrix(seq)
gain <- newseq[ind:(ind+len),allele]
newseq <- as.data.frame(newseq)
newseq[,(ncol(seq)+1)] <- rep(0, nrow(newseq))
newseq <- as.matrix(newseq)
newseq[ind:(ind+len),(ncol(seq)+1)] <- gain
newseq
}
#by default we'll assume losses and gains are equally likely
CNVfun <- function(clone, probvec=c(.5, .5)){
sam <- sample(1:2, 1, prob=probvec)
if(sam==1){
newclone <- lossfun(clone)
}else{
newclone <- gainfun(clone)
}
newclone
}
clones <- list(basesnp)
while((length(clones)-1) < hclones) {
i <- sample(1, 1:length(clones))
baseclone <- clones[[i]]
newclone <- CNVfun(baseclone)
clones <- unlist(list(clones, list(newclone)), recursive=FALSE)
}
clones <- clones[2:length(clones)]
extant <- clones[sample(1:length(clones), nclones)]
#Generating theoretical LRR and BAF
BAFfun <- function(clone){
df <- extant[[clone]]
innerfun <- function(i){
row <- unname(df[i,])
row <- row[row!="0"]
length(row[row=="B"])/length(row)
}
psis[clone]*sapply(1:nrow(df), innerfun)
}
LRRfun <- function(clone){
df <- extant[[clone]]
innerfun <- function(i){
row <- unname(df[i,])
row <- row[row!="0"]
log10(length(row)/2)
}
psis[clone]*sapply(1:nrow(df), innerfun)
}
BAF <- Reduce("+", lapply(1:nclones, BAFfun))
LRR <- Reduce("+", lapply(1:nclones, LRRfun))
#Introducing noise:
estBetaParams <- function(mu, s2) {
temp <- mu*(1-mu)/s2 - 1
alpha <- mu*temp
beta <- (1 - mu)*temp
abs(c(alpha = alpha, beta = beta))
}
bafgen <- function(i, s2=.02){
mu <- BAF[i]
bafs <- rnorm(1, mu, s2)
bafs[bafs < 0] <- -bafs[bafs < 0]
bafs[bafs > 1] <- 1 - (bafs[bafs > 1]-1)
bafs
}
dat.baf <- sapply(1:length(BAF), bafgen)
lrrgen <- function(i, s2=.02){
mu <- LRR[i]
rnorm(1, mu, s2)
}
dat.lrr <- sapply(1:length(LRR), lrrgen)
plot(dat.lrr, cex=.01)
plot(dat.baf, cex=.01)
#Segmentation:
v <- var(dat.lrr)
m <- median(dat.lrr)
breakfun <- function(i, interval=25){
mdiff <- abs(median(dat.lrr[(i-interval):i]) - median(dat.lrr[i:(i+interval)]))
sddiff<- abs(sd(dat.lrr[(i-interval):i]) - sd(dat.lrr[i:(i+interval)]))
mdiff/sddiff
}
ratios <- sapply(51:(l-50), breakfun)
plot(ratios, type="l")
mean(ratios)
threshold <- 3*sd(ratios)
changepoints <- which(ratios>threshold)
startvec <- c(1, changepoints)
endvec <- c(changepoints, l)
yvec <- sapply(1:length(startvec), function(i){median(dat.lrr[startvec[i]:endvec[i]])})
plot(dat.lrr, cex=.01)
segments(x0=startvec, y0=yvec,, x1=endvec, col="green")
markers <- endvec-startvec
med.lrr <- yvec
dat.baf.fold <- dat.baf
dat.baf.fold[dat.baf.fold>.5] <- 1 - dat.baf.fold[dat.baf.fold>.5]
med.baf <- sapply(1:length(startvec), function(i){
median(dat.baf.fold[startvec[i]:endvec[i]])})
list(med.lrr, med.baf, markers)
library(DeepCNV)
library(gtools)
library(combinat)
#library(CloneFinder)
source("F:\\Methods2\\objs4.r")
set.seed(539121)
# pure centers
xy <- data.frame(x = log10(c(2, 2, 1, 3, 4)/2),
y = c(1/2, 0, 0, 1/3, 1/4))
plot(xy, pch=16)
data <- read.table(file="C:\\Users\\Mark\\Documents\\Lab\\SNP_Array_data_New\\Data_CLL.txt")
sam <- data[data$SamID=="CL043",]
nSeg <- nrow(sam)
install.packages("gtools", "combinat")
install.packages("Rgraphviz")
install.packages("diagram")
library(diagram)
demo(flowchart)
source("F:\\Methods2\\CloneFinderFolder\\00-generics.r")
source("F:\\Methods2\\CloneFinderFolder\\01-cloneMaker.r")
source("F:\\Methods2\\CloneFinderFolder\\02-prefit.r")
source("F:\\Methods2\\CloneFinderFolder\\03-psi.r")
library(gtools)
weights=rep(1/5, 5)
fracs <- c(1, 3, 4)
nSegments <- 1000
obj <- Clone(nSegments)
tumor <- Tumor(obj, fracs, wts)
wts=rep(1/5, 5)
tumor <- Tumor(obj, fracs, wts)
slotNames(obj)
markers <- round(runif(nSegments, 25, 1000))
sigma0 <- .25
markers <- round(runif(nSegments, 25, 1000))
xy <- data.frame(x = log10(c(2, 2, 1, 3, 4)/2),
y = c(1/2, 0, 0, 1/3, 1/4))
obj <- compartmentModel(markers, xy, sigma0)
obj <- CompartmentModel(markers, xy, sigma0)
tumor <- Tumor(obj, fracs, wts)
data <- generateData(tumor)
compModel <- CompartmentModel(markers, xy, sigma0)
tumor <- Tumor(compModel, fracs, wts)
dataset <- generateData(tumor)
pcm <- PrefitCloneModel(dataset, compModel)
upd <- updatePhiVectors(pcm, compModel)
estpsi <- guessPsi(upd, 3) # 3 is number of clones we are trying to fit
final <- runEMalg(estpsi, dataset, compModel)
class(final)
length(final)
estpsi
compModel
dataset
min(dataset)
min(dataset[,1])
min(dataset[,2])
dataset[,2][dataset[,2]<0] <- -dataset[,2][dataset[,2]<0]
dataset
min(dataset[,2])
pcm <- PrefitCloneModel(dataset, compModel)
# update it
upd <- updatePhiVectors(pcm, compModel)
# good guess at psi-vector
estpsi <- guessPsi(upd, 3) # 3 is number of clones we are trying to fit
final <- runEMalg(estpsi, dataset, compModel)
compartments <- compModel
Zmats
epsilon=100
ctrl=list(trace=1, reltol=1e-4)
nclone <- length(estpsi)
meth <- ifelse(nclone==2, "Brent", "Nelder-Mead")
zedary <- precomputeZed(5, nclone)
Zmats <- setZs(estpsi, zedary, dataset, compartments) # initialize Z's
currlike <- 0
lastlike <- -10^5
runner <- optim(rep(0, nclone - 1), myTarget, Zs=Zmats, data=dataset, compartments=compartments,
method=meth, control=ctrl)
trial <- myTarget(c(.3, .5), Zmats, data, copmartments)
trial <- myTarget(c(.3, .5), Zmats, data, compartments)
trial
trial <- myTarget(c(.3, .1), Zmats, data, compartments)
trial
trial <- myTarget(c(.3, .7), Zmats, data, compartments)
trial
trial <- myTarget(c(1, 2), Zmats, data, compartments)
trial
trial <- myTarget(c(3, 2), Zmats, data, compartments)
trial
trial <- myTarget(c(.1, .5), Zmats, data, compartments)
Zs <- Zmats
data <- dataset
x <- c(.1, 5)
psi <- backward(x)
if (length(psi) != dim(Zs)[3]) stop('mismatched argmuent sizes')
# next formula is vectorization of " phi = sum_i (psi_i * Z_i) "
phinew <- apply(sweep(Zs, 3, psi, "*"), 1:2, sum)
dim(phinew)
phinew
loglikes <- sum(tock <- sapply(1:nrow(phinew), function(i, phi) {
ci <- compartments
ci@markers <- compartments@markers[i]
sum(log(likely(data[i,], phi[i,], ci)))
}, phi=phinew))
x <- c(0, 0)
x
psi <- backward(x)
if (length(psi) != dim(Zs)[3]) stop('mismatched argmuent sizes')
phinew <- apply(sweep(Zs, 3, psi, "*"), 1:2, sum)
dim(phinew)
phinew
loglikes <- sum(tock <- sapply(1:nrow(phinew), function(i, phi) {
ci <- compartments
ci@markers <- compartments@markers[i]
sum(log(likely(data[i,], phi[i,], ci)))
}, phi=phinew))
- loglikes  # negate
fun <- function(i, phi) {
ci <- compartments
ci@markers <- compartments@markers[i]
sum(log(likely(data[i,], phi[i,], ci)))
}
funtrial <- fun(1, phinew)
funtrial
obj <- sapply(1:nrow(phinew), fun, phi=phinew)
obj
loglikes
-loglikes
library(CloneFinder)
setwd("C:\\Users\\Mark\\Documents\\R-Forge\\CheckoutFolder-clonefinder\\pkg\\CloneFinder\\tests")
source("tst04-Psi.r")
library(CloneFinder)
setwd("C:\\Users\\Mark\\Documents\\R-Forge\\CheckoutFolder-clonefinder\\pkg\\CloneFinder\\tests")
source("tst04-Psi.r")
